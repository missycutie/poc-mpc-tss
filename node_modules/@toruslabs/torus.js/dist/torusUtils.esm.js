import _defineProperty from '@babel/runtime/helpers/defineProperty';
import BN$1, { BN } from 'bn.js';
import { KEY_TYPE, LEGACY_NETWORKS_ROUTE_MAP, TORUS_SAPPHIRE_NETWORK, SIGNER_MAP, METADATA_MAP } from '@toruslabs/constants';
import { post, generateJsonRPCObject, get, setAPIKey, setEmbedHost } from '@toruslabs/http-helpers';
import { ec } from 'elliptic';
import log$1 from 'loglevel';
import _objectSpread from '@babel/runtime/helpers/objectSpread2';
import { decrypt, encrypt, generatePrivate, getPublic } from '@toruslabs/eccrypto';
import { getRandomBytes } from 'ethereum-cryptography/random';
import { keccak256 as keccak256$1 } from 'ethereum-cryptography/keccak';
import stringify from 'json-stable-stringify';
import { bs58 } from '@toruslabs/bs58';
import { sha512 } from 'ethereum-cryptography/sha512';

const JRPC_METHODS = {
  GET_OR_SET_KEY: "GetPubKeyOrKeyAssign",
  VERIFIER_LOOKUP: "VerifierLookupRequest",
  COMMITMENT_REQUEST: "CommitmentRequest",
  IMPORT_SHARES: "ImportShares",
  GET_SHARE_OR_KEY_ASSIGN: "GetShareOrKeyAssign"
};
const SAPPHIRE_METADATA_URL = "https://node-1.node.web3auth.io/metadata";
const SAPPHIRE_DEVNET_METADATA_URL = "https://node-1.dev-node.web3auth.io/metadata";

function keccak256(a) {
  const hash = Buffer.from(keccak256$1(a)).toString("hex");
  return `0x${hash}`;
}
const generatePrivateKey = (ecCurve, buf) => {
  return ecCurve.genKeyPair().getPrivate().toArrayLike(buf);
};
const getKeyCurve = keyType => {
  if (keyType === KEY_TYPE.ED25519) {
    return new ec(KEY_TYPE.ED25519);
  } else if (keyType === KEY_TYPE.SECP256K1) {
    return new ec(KEY_TYPE.SECP256K1);
  }
  throw new Error(`Invalid keyType: ${keyType}`);
};
// this function normalizes the result from nodes before passing the result to threshold check function
// For ex: some fields returns by nodes might be different from each other
// like created_at field might vary and nonce_data might not be returned by all nodes because
// of the metadata implementation in sapphire.
const normalizeKeysResult = result => {
  const finalResult = {
    keys: [],
    is_new_key: result.is_new_key
  };
  if (result && result.keys && result.keys.length > 0) {
    const finalKey = result.keys[0];
    finalResult.keys = [{
      pub_key_X: finalKey.pub_key_X,
      pub_key_Y: finalKey.pub_key_Y,
      address: finalKey.address
    }];
  }
  return finalResult;
};
const normalizeLookUpResult = result => {
  const finalResult = {
    keys: []
  };
  if (result && result.keys && result.keys.length > 0) {
    const finalKey = result.keys[0];
    finalResult.keys = [{
      pub_key_X: finalKey.pub_key_X,
      pub_key_Y: finalKey.pub_key_Y,
      address: finalKey.address
    }];
  }
  return finalResult;
};
const kCombinations = (s, k) => {
  let set = s;
  if (typeof set === "number") {
    set = Array.from({
      length: set
    }, (_, i) => i);
  }
  if (k > set.length || k <= 0) {
    return [];
  }
  if (k === set.length) {
    return [set];
  }
  if (k === 1) {
    return set.reduce((acc, cur) => [...acc, [cur]], []);
  }
  const combs = [];
  let tailCombs = [];
  for (let i = 0; i <= set.length - k + 1; i += 1) {
    tailCombs = kCombinations(set.slice(i + 1), k - 1);
    for (let j = 0; j < tailCombs.length; j += 1) {
      combs.push([set[i], ...tailCombs[j]]);
    }
  }
  return combs;
};
const thresholdSame = (arr, t) => {
  const hashMap = {};
  for (let i = 0; i < arr.length; i += 1) {
    const str = stringify(arr[i]);
    hashMap[str] = hashMap[str] ? hashMap[str] + 1 : 1;
    if (hashMap[str] === t) {
      return arr[i];
    }
  }
  return undefined;
};
function encParamsBufToHex(encParams) {
  return {
    iv: Buffer.from(encParams.iv).toString("hex"),
    ephemPublicKey: Buffer.from(encParams.ephemPublicKey).toString("hex"),
    ciphertext: Buffer.from(encParams.ciphertext).toString("hex"),
    mac: Buffer.from(encParams.mac).toString("hex"),
    mode: "AES256"
  };
}
function encParamsHexToBuf(eciesData) {
  return {
    ephemPublicKey: Buffer.from(eciesData.ephemPublicKey, "hex"),
    iv: Buffer.from(eciesData.iv, "hex"),
    mac: Buffer.from(eciesData.mac, "hex")
  };
}
function getProxyCoordinatorEndpointIndex(endpoints, verifier, verifierId) {
  const verifierIdStr = `${verifier}${verifierId}`;
  const hashedVerifierId = keccak256(Buffer.from(verifierIdStr, "utf8")).slice(2);
  const proxyEndpointNum = new BN(hashedVerifierId, "hex").mod(new BN(endpoints.length)).toNumber();
  return proxyEndpointNum;
}
function calculateMedian(arr) {
  const arrSize = arr.length;
  if (arrSize === 0) return 0;
  const sortedArr = arr.sort(function (a, b) {
    return a - b;
  });

  // odd length
  if (arrSize % 2 !== 0) {
    return sortedArr[Math.floor(arrSize / 2)];
  }

  // return average of two mid values in case of even arrSize
  const mid1 = sortedArr[arrSize / 2 - 1];
  const mid2 = sortedArr[arrSize / 2];
  return (mid1 + mid2) / 2;
}
function waitFor(milliseconds) {
  return new Promise((resolve, reject) => {
    // hack to bypass eslint warning.
    if (milliseconds > 0) {
      setTimeout(resolve, milliseconds);
    } else {
      reject(new Error("value of milliseconds must be greater than 0"));
    }
  });
}
function retryCommitment(executionPromise, maxRetries) {
  // Notice that we declare an inner function here
  // so we can encapsulate the retries and don't expose
  // it to the caller. This is also a recursive function
  async function retryWithBackoff(retries) {
    try {
      // we don't wait on the first attempt
      if (retries > 0) {
        // on every retry, we exponentially increase the time to wait.
        // Here is how it looks for a `maxRetries` = 4
        // (2 ** 1) * 100 = 200 ms
        // (2 ** 2) * 100 = 400 ms
        // (2 ** 3) * 100 = 800 ms
        const timeToWait = 2 ** retries * 100;
        await waitFor(timeToWait);
      }
      const a = await executionPromise();
      return a;
    } catch (e) {
      const errorMsg = e.message;
      const acceptedErrorMsgs = [
      // Slow node
      "Timed out", "Failed to fetch", "fetch failed", "Load failed", "cancelled", "NetworkError when attempting to fetch resource.",
      // Happens when the node is not reachable (dns issue etc)
      "TypeError: Failed to fetch",
      // All except iOS and Firefox
      "TypeError: cancelled",
      // iOS
      "TypeError: NetworkError when attempting to fetch resource." // Firefox
      ];
      if (retries < maxRetries && (acceptedErrorMsgs.includes(errorMsg) || errorMsg && errorMsg.includes("reason: getaddrinfo EAI_AGAIN"))) {
        // only retry if we didn't reach the limit
        // otherwise, let the caller handle the error
        return retryWithBackoff(retries + 1);
      }
      throw e;
    }
  }
  return retryWithBackoff(0);
}

class GetOrSetNonceError extends Error {}

class Point {
  constructor(x, y, ecCurve) {
    _defineProperty(this, "x", void 0);
    _defineProperty(this, "y", void 0);
    _defineProperty(this, "ecCurve", void 0);
    this.x = new BN$1(x, "hex");
    this.y = new BN$1(y, "hex");
    this.ecCurve = ecCurve;
  }
  encode(enc) {
    switch (enc) {
      case "arr":
        return Buffer.concat([Buffer.from("04", "hex"), Buffer.from(this.x.toString("hex", 64), "hex"), Buffer.from(this.y.toString("hex", 64), "hex")]);
      case "elliptic-compressed":
        {
          const key = this.ecCurve.keyFromPublic({
            x: this.x.toString("hex", 64),
            y: this.y.toString("hex", 64)
          }, "hex");
          return Buffer.from(key.getPublic(true, "hex"));
        }
      default:
        throw new Error("encoding doesn't exist in Point");
    }
  }
}

class Share {
  constructor(shareIndex, share) {
    _defineProperty(this, "share", void 0);
    _defineProperty(this, "shareIndex", void 0);
    this.share = new BN$1(share, "hex");
    this.shareIndex = new BN$1(shareIndex, "hex");
  }
  static fromJSON(value) {
    const {
      share,
      shareIndex
    } = value;
    return new Share(shareIndex, share);
  }
  toJSON() {
    return {
      share: this.share.toString("hex", 64),
      shareIndex: this.shareIndex.toString("hex", 64)
    };
  }
}

class Polynomial {
  constructor(polynomial, ecCurve) {
    _defineProperty(this, "polynomial", void 0);
    _defineProperty(this, "ecCurve", void 0);
    this.polynomial = polynomial;
    this.ecCurve = ecCurve;
  }
  getThreshold() {
    return this.polynomial.length;
  }
  polyEval(x) {
    const tmpX = new BN$1(x, "hex");
    let xi = new BN$1(tmpX);
    let sum = new BN$1(0);
    sum = sum.add(this.polynomial[0]);
    for (let i = 1; i < this.polynomial.length; i += 1) {
      const tmp = xi.mul(this.polynomial[i]);
      sum = sum.add(tmp);
      sum = sum.umod(this.ecCurve.n);
      xi = xi.mul(new BN$1(tmpX));
      xi = xi.umod(this.ecCurve.n);
    }
    return sum;
  }
  generateShares(shareIndexes) {
    const newShareIndexes = shareIndexes.map(index => {
      if (typeof index === "number") {
        return new BN$1(index);
      }
      if (index instanceof BN$1) {
        return index;
      }
      if (typeof index === "string") {
        return new BN$1(index, "hex");
      }
      return index;
    });
    const shares = {};
    for (let x = 0; x < newShareIndexes.length; x += 1) {
      shares[newShareIndexes[x].toString("hex", 64)] = new Share(newShareIndexes[x], this.polyEval(newShareIndexes[x]));
    }
    return shares;
  }
}

function generatePrivateExcludingIndexes(shareIndexes, ecCurve) {
  const key = new BN$1(generatePrivateKey(ecCurve, Buffer));
  if (shareIndexes.find(el => el.eq(key))) {
    return generatePrivateExcludingIndexes(shareIndexes, ecCurve);
  }
  return key;
}
const generateEmptyBNArray = length => Array.from({
  length
}, () => new BN$1(0));
const denominator = (ecCurve, i, innerPoints) => {
  let result = new BN$1(1);
  const xi = innerPoints[i].x;
  for (let j = innerPoints.length - 1; j >= 0; j -= 1) {
    if (i !== j) {
      let tmp = new BN$1(xi);
      tmp = tmp.sub(innerPoints[j].x);
      tmp = tmp.umod(ecCurve.n);
      result = result.mul(tmp);
      result = result.umod(ecCurve.n);
    }
  }
  return result;
};
const interpolationPoly = (ecCurve, i, innerPoints) => {
  let coefficients = generateEmptyBNArray(innerPoints.length);
  const d = denominator(ecCurve, i, innerPoints);
  if (d.cmp(new BN$1(0)) === 0) {
    throw new Error("Denominator for interpolationPoly is 0");
  }
  coefficients[0] = d.invm(ecCurve.n);
  for (let k = 0; k < innerPoints.length; k += 1) {
    const newCoefficients = generateEmptyBNArray(innerPoints.length);
    if (k !== i) {
      let j;
      if (k < i) {
        j = k + 1;
      } else {
        j = k;
      }
      j -= 1;
      for (; j >= 0; j -= 1) {
        newCoefficients[j + 1] = newCoefficients[j + 1].add(coefficients[j]).umod(ecCurve.n);
        let tmp = new BN$1(innerPoints[k].x);
        tmp = tmp.mul(coefficients[j]).umod(ecCurve.n);
        newCoefficients[j] = newCoefficients[j].sub(tmp).umod(ecCurve.n);
      }
      coefficients = newCoefficients;
    }
  }
  return coefficients;
};
const pointSort = innerPoints => {
  const pointArrClone = [...innerPoints];
  pointArrClone.sort((a, b) => a.x.cmp(b.x));
  return pointArrClone;
};
const lagrange = (ecCurve, unsortedPoints) => {
  const sortedPoints = pointSort(unsortedPoints);
  const polynomial = generateEmptyBNArray(sortedPoints.length);
  for (let i = 0; i < sortedPoints.length; i += 1) {
    const coefficients = interpolationPoly(ecCurve, i, sortedPoints);
    for (let k = 0; k < sortedPoints.length; k += 1) {
      let tmp = new BN$1(sortedPoints[i].y);
      tmp = tmp.mul(coefficients[k]);
      polynomial[k] = polynomial[k].add(tmp).umod(ecCurve.n);
    }
  }
  return new Polynomial(polynomial, ecCurve);
};
function lagrangeInterpolatePolynomial(ecCurve, points) {
  return lagrange(ecCurve, points);
}
function lagrangeInterpolation(ecCurve, shares, nodeIndex) {
  if (shares.length !== nodeIndex.length) {
    throw new Error("shares not equal to nodeIndex length in lagrangeInterpolation");
  }
  let secret = new BN$1(0);
  for (let i = 0; i < shares.length; i += 1) {
    let upper = new BN$1(1);
    let lower = new BN$1(1);
    for (let j = 0; j < shares.length; j += 1) {
      if (i !== j) {
        upper = upper.mul(nodeIndex[j].neg());
        upper = upper.umod(ecCurve.n);
        let temp = nodeIndex[i].sub(nodeIndex[j]);
        temp = temp.umod(ecCurve.n);
        lower = lower.mul(temp).umod(ecCurve.n);
      }
    }
    let delta = upper.mul(lower.invm(ecCurve.n)).umod(ecCurve.n);
    delta = delta.mul(shares[i]).umod(ecCurve.n);
    secret = secret.add(delta);
  }
  return secret.umod(ecCurve.n);
}

// generateRandomPolynomial - determinisiticShares are assumed random
function generateRandomPolynomial(ecCurve, degree, secret, deterministicShares) {
  let actualS = secret;
  if (!secret) {
    actualS = generatePrivateExcludingIndexes([new BN$1(0)], ecCurve);
  }
  if (!deterministicShares) {
    const poly = [actualS];
    for (let i = 0; i < degree; i += 1) {
      const share = generatePrivateExcludingIndexes(poly, ecCurve);
      poly.push(share);
    }
    return new Polynomial(poly, ecCurve);
  }
  if (!Array.isArray(deterministicShares)) {
    throw new Error("deterministic shares in generateRandomPolynomial should be an array");
  }
  if (deterministicShares.length > degree) {
    throw new Error("deterministicShares in generateRandomPolynomial should be less or equal than degree to ensure an element of randomness");
  }
  const points = {};
  deterministicShares.forEach(share => {
    points[share.shareIndex.toString("hex", 64)] = new Point(share.shareIndex, share.share, ecCurve);
  });
  for (let i = 0; i < degree - deterministicShares.length; i += 1) {
    let shareIndex = generatePrivateExcludingIndexes([new BN$1(0)], ecCurve);
    while (points[shareIndex.toString("hex", 64)] !== undefined) {
      shareIndex = generatePrivateExcludingIndexes([new BN$1(0)], ecCurve);
    }
    points[shareIndex.toString("hex", 64)] = new Point(shareIndex, new BN$1(generatePrivateKey(ecCurve, Buffer)), ecCurve);
  }
  points["0"] = new Point(new BN$1(0), actualS, ecCurve);
  return lagrangeInterpolatePolynomial(ecCurve, Object.values(points));
}

const getSecpKeyFromEd25519 = ed25519Scalar => {
  const secp256k1Curve = getKeyCurve(KEY_TYPE.SECP256K1);
  const ed25519Key = ed25519Scalar.toString("hex", 64);
  const keyHash = keccak256$1(Buffer.from(ed25519Key, "hex"));
  const secpKey = new BN$1(keyHash).umod(secp256k1Curve.n).toString("hex", 64);
  const bufferKey = Buffer.from(secpKey, "hex");
  const secpKeyPair = secp256k1Curve.keyFromPrivate(bufferKey);
  if (bufferKey.length !== 32) {
    throw new Error(`Key length must be equal to 32. got ${bufferKey.length}`);
  }
  return {
    scalar: secpKeyPair.getPrivate(),
    point: secpKeyPair.getPublic()
  };
};
function convertMetadataToNonce(params) {
  if (!params || !params.message) {
    return new BN$1(0);
  }
  return new BN$1(params.message, 16);
}
async function decryptNodeData(eciesData, ciphertextHex, privKey) {
  const metadata = encParamsHexToBuf(eciesData);
  const decryptedSigBuffer = await decrypt(privKey, _objectSpread(_objectSpread({}, metadata), {}, {
    ciphertext: Buffer.from(ciphertextHex, "hex")
  }));
  return decryptedSigBuffer;
}
async function decryptNodeDataWithPadding(eciesData, ciphertextHex, privKey) {
  const metadata = encParamsHexToBuf(eciesData);
  try {
    const decryptedSigBuffer = await decrypt(privKey, _objectSpread(_objectSpread({}, metadata), {}, {
      ciphertext: Buffer.from(ciphertextHex, "hex")
    }));
    return decryptedSigBuffer;
  } catch (error) {
    // ciphertext can be any length. not just 64. depends on input. we have this for legacy reason
    const ciphertextHexPadding = ciphertextHex.padStart(64, "0");
    log$1.warn("Failed to decrypt padded share cipher", error);
    // try without cipher text padding
    return decrypt(privKey, _objectSpread(_objectSpread({}, metadata), {}, {
      ciphertext: Buffer.from(ciphertextHexPadding, "hex")
    }));
  }
}
function generateMetadataParams(ecCurve, serverTimeOffset, message, privateKey) {
  const key = ecCurve.keyFromPrivate(privateKey.toString("hex", 64), "hex");
  const setData = {
    data: message,
    timestamp: new BN$1(~~(serverTimeOffset + Date.now() / 1000)).toString(16)
  };
  const sig = key.sign(keccak256(Buffer.from(stringify(setData), "utf8")).slice(2));
  return {
    pub_key_X: key.getPublic().getX().toString("hex"),
    // DO NOT PAD THIS. BACKEND DOESN'T
    pub_key_Y: key.getPublic().getY().toString("hex"),
    // DO NOT PAD THIS. BACKEND DOESN'T
    set_data: setData,
    signature: Buffer.from(sig.r.toString(16, 64) + sig.s.toString(16, 64) + new BN$1("").toString(16, 2), "hex").toString("base64")
  };
}
async function getMetadata(legacyMetadataHost, data, options = {}) {
  try {
    const metadataResponse = await post(`${legacyMetadataHost}/get`, data, options, {
      useAPIKey: true
    });
    if (!metadataResponse || !metadataResponse.message) {
      return new BN$1(0);
    }
    return new BN$1(metadataResponse.message, 16); // nonce
  } catch (error) {
    log$1.error("get metadata error", error);
    return new BN$1(0);
  }
}
function generateNonceMetadataParams(serverTimeOffset, operation, privateKey, keyType, nonce, seed) {
  // metadata only uses secp for sig validation
  const key = getKeyCurve(KEY_TYPE.SECP256K1).keyFromPrivate(privateKey.toString("hex", 64), "hex");
  const setData = {
    operation,
    timestamp: new BN$1(~~(serverTimeOffset + Date.now() / 1000)).toString(16)
  };
  if (nonce) {
    setData.data = nonce.toString("hex", 64);
  }
  if (seed) {
    setData.seed = seed;
  } else {
    setData.seed = ""; // setting it as empty to keep ordering same while serializing the data on backend.
  }
  const sig = key.sign(keccak256(Buffer.from(stringify(setData), "utf8")).slice(2));
  return {
    pub_key_X: key.getPublic().getX().toString("hex", 64),
    pub_key_Y: key.getPublic().getY().toString("hex", 64),
    set_data: setData,
    key_type: keyType,
    signature: Buffer.from(sig.r.toString(16, 64) + sig.s.toString(16, 64) + new BN$1("").toString(16, 2), "hex").toString("base64")
  };
}
async function getOrSetNonce(metadataHost, ecCurve, serverTimeOffset, X, Y, privKey, getOnly = false, isLegacyMetadata = true, nonce = new BN$1(0), keyType = "secp256k1", seed = "") {
  // for legacy metadata
  if (isLegacyMetadata) {
    let data;
    const msg = getOnly ? "getNonce" : "getOrSetNonce";
    if (privKey) {
      data = generateMetadataParams(ecCurve, serverTimeOffset, msg, privKey);
    } else {
      data = {
        pub_key_X: X,
        pub_key_Y: Y,
        set_data: {
          data: msg
        }
      };
    }
    return post(`${metadataHost}/get_or_set_nonce`, data, undefined, {
      useAPIKey: true
    });
  }

  // for sapphire metadata
  const operation = getOnly ? "getNonce" : "getOrSetNonce";
  if (operation === "getOrSetNonce") {
    if (!privKey) {
      throw new Error("privKey is required while `getOrSetNonce` for non legacy metadata");
    }
    if (nonce.cmp(new BN$1(0)) === 0) {
      throw new Error("nonce is required while `getOrSetNonce` for non legacy metadata");
    }
    if (keyType === KEY_TYPE.ED25519 && !seed) {
      throw new Error("seed is required while `getOrSetNonce` for non legacy metadata for ed25519 key type");
    }
    const data = generateNonceMetadataParams(serverTimeOffset, operation, privKey, keyType, nonce, seed);
    return post(`${metadataHost}/get_or_set_nonce`, data, undefined, {
      useAPIKey: true
    });
  }
  const data = {
    pub_key_X: X,
    pub_key_Y: Y,
    set_data: {
      operation
    },
    key_type: keyType
  };
  return post(`${metadataHost}/get_or_set_nonce`, data, undefined, {
    useAPIKey: true
  });
}
async function getNonce(legacyMetadataHost, ecCurve, serverTimeOffset, X, Y, privKey) {
  return getOrSetNonce(legacyMetadataHost, ecCurve, serverTimeOffset, X, Y, privKey, true);
}
const decryptSeedData = async (seedBase64, finalUserKey) => {
  const decryptionKey = getSecpKeyFromEd25519(finalUserKey);
  const seedUtf8 = Buffer.from(seedBase64, "base64").toString("utf-8");
  const seedJson = JSON.parse(seedUtf8);
  const bufferMetadata = _objectSpread(_objectSpread({}, encParamsHexToBuf(seedJson.metadata)), {}, {
    mode: "AES256"
  });
  const bufferKey = decryptionKey.scalar.toArrayLike(Buffer, "be", 32);
  const decText = await decrypt(bufferKey, _objectSpread(_objectSpread({}, bufferMetadata), {}, {
    ciphertext: Buffer.from(seedJson.enc_text, "hex")
  }));
  return decText;
};
async function getOrSetSapphireMetadataNonce(network, X, Y, serverTimeOffset, privKey) {
  if (LEGACY_NETWORKS_ROUTE_MAP[network]) {
    throw new Error("getOrSetSapphireMetadataNonce should only be used for sapphire networks");
  }
  let data = {
    pub_key_X: X,
    pub_key_Y: Y,
    key_type: "secp256k1",
    set_data: {
      operation: "getOrSetNonce"
    }
  };
  if (privKey) {
    const key = getKeyCurve(KEY_TYPE.SECP256K1).keyFromPrivate(privKey.toString("hex", 64), "hex");
    const setData = {
      operation: "getOrSetNonce",
      timestamp: new BN$1(~~(serverTimeOffset + Date.now() / 1000)).toString(16)
    };
    const sig = key.sign(keccak256(Buffer.from(stringify(setData), "utf8")).slice(2));
    data = _objectSpread(_objectSpread({}, data), {}, {
      set_data: setData,
      signature: Buffer.from(sig.r.toString(16, 64) + sig.s.toString(16, 64) + new BN$1("").toString(16, 2), "hex").toString("base64")
    });
  }
  const metadataUrl = network === TORUS_SAPPHIRE_NETWORK.SAPPHIRE_DEVNET ? SAPPHIRE_DEVNET_METADATA_URL : SAPPHIRE_METADATA_URL;
  return post(`${metadataUrl}/get_or_set_nonce`, data, undefined, {
    useAPIKey: true
  });
}

function stripHexPrefix(str) {
  return str.startsWith("0x") ? str.slice(2) : str;
}
function toChecksumAddress(hexAddress) {
  const address = stripHexPrefix(hexAddress).toLowerCase();
  const buf = Buffer.from(address, "utf8");
  const hash = Buffer.from(keccak256$1(buf)).toString("hex");
  let ret = "0x";
  for (let i = 0; i < address.length; i++) {
    if (parseInt(hash[i], 16) >= 8) {
      ret += address[i].toUpperCase();
    } else {
      ret += address[i];
    }
  }
  return ret;
}
function adjustScalarBytes(bytes) {
  // Section 5: For X25519, in order to decode 32 random bytes as an integer scalar,
  // set the three least significant bits of the first byte
  bytes[0] &= 248; // 0b1111_1000
  // and the most significant bit of the last to zero,
  bytes[31] &= 127; // 0b0111_1111
  // set the second most significant bit of the last byte to 1
  bytes[31] |= 64; // 0b0100_0000
  return bytes;
}

/** Convenience method that creates public key and other stuff. RFC8032 5.1.5 */
function getEd25519ExtendedPublicKey(keyBuffer) {
  const ed25519Curve = getKeyCurve(KEY_TYPE.ED25519);
  const len = 32;
  const G = ed25519Curve.g;
  const N = ed25519Curve.n;
  if (keyBuffer.length !== 32) {
    log$1.error("Invalid seed for ed25519 key derivation", keyBuffer.length);
    throw new Error("Invalid seed for ed25519 key derivation");
  }
  // Hash private key with curve's hash function to produce uniformingly random input
  // Check byte lengths: ensure(64, h(ensure(32, key)))
  const hashed = sha512(keyBuffer);
  if (hashed.length !== 64) {
    throw new Error("Invalid hash length for ed25519 seed");
  }
  const head = new BN$1(adjustScalarBytes(Buffer.from(hashed.slice(0, len))), "le");
  const scalar = new BN$1(head.umod(N), "le"); // The actual private scalar
  const point = G.mul(scalar); // Point on Edwards curve aka public key
  return {
    scalar,
    point
  };
}
function encodeEd25519Point(point) {
  const ed25519Curve = getKeyCurve(KEY_TYPE.ED25519);
  const encodingLength = Math.ceil(ed25519Curve.n.bitLength() / 8);
  const enc = point.getY().toArrayLike(Buffer, "le", encodingLength);
  enc[encodingLength - 1] |= point.getX().isOdd() ? 0x80 : 0;
  return enc;
}
const generateEd25519KeyData = async ed25519Seed => {
  const ed25519Curve = getKeyCurve(KEY_TYPE.ED25519);
  const finalEd25519Key = getEd25519ExtendedPublicKey(ed25519Seed);
  const encryptionKey = getSecpKeyFromEd25519(finalEd25519Key.scalar);
  const encryptedSeed = await encrypt(Buffer.from(encryptionKey.point.encodeCompressed("hex"), "hex"), ed25519Seed);
  const encData = {
    enc_text: encryptedSeed.ciphertext.toString("hex"),
    metadata: encParamsBufToHex(encryptedSeed),
    public_key: encodeEd25519Point(finalEd25519Key.point).toString("hex")
  };
  const encDataBase64 = Buffer.from(JSON.stringify(encData), "utf-8").toString("base64");
  const metadataPrivNonce = ed25519Curve.genKeyPair().getPrivate();
  const oauthKey = finalEd25519Key.scalar.sub(metadataPrivNonce).umod(ed25519Curve.n);
  const oauthKeyPair = ed25519Curve.keyFromPrivate(oauthKey.toArrayLike(Buffer));
  const metadataSigningKey = getSecpKeyFromEd25519(oauthKeyPair.getPrivate());
  return {
    oAuthKeyScalar: oauthKeyPair.getPrivate(),
    oAuthPubX: oauthKeyPair.getPublic().getX(),
    oAuthPubY: oauthKeyPair.getPublic().getY(),
    SigningPubX: metadataSigningKey.point.getX(),
    SigningPubY: metadataSigningKey.point.getY(),
    metadataNonce: metadataPrivNonce,
    metadataSigningKey: metadataSigningKey.scalar,
    encryptedSeed: encDataBase64,
    finalUserPubKeyPoint: finalEd25519Key.point
  };
};
const generateSecp256k1KeyData = async scalarBuffer => {
  const secp256k1Curve = getKeyCurve(KEY_TYPE.SECP256K1);
  const scalar = new BN$1(scalarBuffer);
  const randomNonce = new BN$1(generatePrivateKey(secp256k1Curve, Buffer));
  const oAuthKey = scalar.sub(randomNonce).umod(secp256k1Curve.n);
  const oAuthKeyPair = secp256k1Curve.keyFromPrivate(oAuthKey.toArrayLike(Buffer));
  const oAuthPubKey = oAuthKeyPair.getPublic();
  const finalUserKeyPair = secp256k1Curve.keyFromPrivate(scalar.toString("hex", 64), "hex");
  return {
    oAuthKeyScalar: oAuthKeyPair.getPrivate(),
    oAuthPubX: oAuthPubKey.getX(),
    oAuthPubY: oAuthPubKey.getY(),
    SigningPubX: oAuthPubKey.getX(),
    SigningPubY: oAuthPubKey.getY(),
    metadataNonce: randomNonce,
    encryptedSeed: "",
    metadataSigningKey: oAuthKeyPair.getPrivate(),
    finalUserPubKeyPoint: finalUserKeyPair.getPublic()
  };
};
function generateAddressFromEcKey(keyType, key) {
  if (keyType === KEY_TYPE.SECP256K1) {
    const publicKey = key.getPublic().encode("hex", false).slice(2);
    const evmAddressLower = `0x${keccak256(Buffer.from(publicKey, "hex")).slice(64 - 38)}`;
    return toChecksumAddress(evmAddressLower);
  } else if (keyType === KEY_TYPE.ED25519) {
    const publicKey = encodeEd25519Point(key.getPublic());
    const address = bs58.encode(publicKey);
    return address;
  }
  throw new Error(`Invalid keyType: ${keyType}`);
}
function generateAddressFromPrivKey(keyType, privateKey) {
  const ecCurve = getKeyCurve(keyType);
  const key = ecCurve.keyFromPrivate(privateKey.toString("hex", 64), "hex");
  return generateAddressFromEcKey(keyType, key);
}
function generateAddressFromPubKey(keyType, publicKeyX, publicKeyY) {
  const ecCurve = getKeyCurve(keyType);
  const key = ecCurve.keyFromPublic({
    x: publicKeyX.toString("hex", 64),
    y: publicKeyY.toString("hex", 64)
  });
  return generateAddressFromEcKey(keyType, key);
}
function getPostboxKeyFrom1OutOf1(ecCurve, privKey, nonce) {
  const privKeyBN = new BN$1(privKey, 16);
  const nonceBN = new BN$1(nonce, 16);
  return privKeyBN.sub(nonceBN).umod(ecCurve.n).toString("hex");
}
function derivePubKey(ecCurve, sk) {
  const skHex = sk.toString(16, 64);
  return ecCurve.keyFromPrivate(skHex, "hex").getPublic();
}
const getEncryptionEC = () => {
  return new ec("secp256k1");
};
const generateShares = async (ecCurve, keyType, serverTimeOffset, nodeIndexes, nodePubkeys, privKey) => {
  const keyData = keyType === KEY_TYPE.ED25519 ? await generateEd25519KeyData(privKey) : await generateSecp256k1KeyData(privKey);
  const {
    metadataNonce,
    oAuthKeyScalar: oAuthKey,
    encryptedSeed,
    metadataSigningKey
  } = keyData;
  const threshold = ~~(nodePubkeys.length / 2) + 1;
  const degree = threshold - 1;
  const nodeIndexesBn = [];
  for (const nodeIndex of nodeIndexes) {
    nodeIndexesBn.push(new BN$1(nodeIndex));
  }
  const oAuthPubKey = ecCurve.keyFromPrivate(oAuthKey.toString("hex", 64), "hex").getPublic();
  const poly = generateRandomPolynomial(ecCurve, degree, oAuthKey);
  const shares = poly.generateShares(nodeIndexesBn);
  const nonceParams = generateNonceMetadataParams(serverTimeOffset, "getOrSetNonce", metadataSigningKey, keyType, metadataNonce, encryptedSeed);
  const nonceData = Buffer.from(stringify(nonceParams.set_data), "utf8").toString("base64");
  const sharesData = [];
  const encPromises = [];
  for (let i = 0; i < nodeIndexesBn.length; i++) {
    const shareJson = shares[nodeIndexesBn[i].toString("hex", 64)].toJSON();
    if (!nodePubkeys[i]) {
      throw new Error(`Missing node pub key for node index: ${nodeIndexesBn[i].toString("hex", 64)}`);
    }
    const nodePubKey = getEncryptionEC().keyFromPublic({
      x: nodePubkeys[i].X,
      y: nodePubkeys[i].Y
    });
    encPromises.push(encrypt(Buffer.from(nodePubKey.getPublic().encodeCompressed("hex"), "hex"), Buffer.from(shareJson.share.padStart(64, "0"), "hex")));
  }
  const encShares = await Promise.all(encPromises);
  for (let i = 0; i < nodeIndexesBn.length; i += 1) {
    const shareJson = shares[nodeIndexesBn[i].toString("hex", 64)].toJSON();
    const encParams = encShares[i];
    const encParamsMetadata = encParamsBufToHex(encParams);
    const shareData = {
      encrypted_seed: keyData.encryptedSeed,
      final_user_point: keyData.finalUserPubKeyPoint,
      oauth_pub_key_x: oAuthPubKey.getX().toString("hex"),
      oauth_pub_key_y: oAuthPubKey.getY().toString("hex"),
      signing_pub_key_x: keyData.SigningPubX.toString("hex"),
      signing_pub_key_y: keyData.SigningPubY.toString("hex"),
      encrypted_share: encParamsMetadata.ciphertext,
      encrypted_share_metadata: encParamsMetadata,
      node_index: Number.parseInt(shareJson.shareIndex, 16),
      key_type: keyType,
      nonce_data: nonceData,
      nonce_signature: nonceParams.signature
    };
    sharesData.push(shareData);
  }
  return sharesData;
};

const config = {
  logRequestTracing: false
};

const log = log$1.getLogger("torus.js");
log.disableAll();

/* eslint-disable promise/catch-or-return */
function capitalizeFirstLetter(str) {
  return str.charAt(0).toUpperCase() + str.slice(1);
}
class SomeError extends Error {
  constructor({
    errors,
    responses,
    predicate
  }) {
    // its fine to log responses in errors logs for better debugging,
    // as data is always encrypted with temp key
    // temp key should not be logged anywhere
    const message = `Unable to resolve enough promises. 
      errors: ${errors.map(x => (x === null || x === void 0 ? void 0 : x.message) || x).join(", ")}, 
      predicate error: ${predicate},
      ${responses.length} responses,
      responses: ${JSON.stringify(responses)}`;
    super(message);
    _defineProperty(this, "errors", void 0);
    _defineProperty(this, "responses", void 0);
    _defineProperty(this, "predicate", void 0);
    this.errors = errors;
    this.responses = responses;
    this.predicate = predicate;
  }
  get message() {
    return `${super.message}. errors: ${this.errors.map(x => (x === null || x === void 0 ? void 0 : x.message) || x).join(", ")} and ${this.responses.length} responses: ${JSON.stringify(this.responses)},
      predicate error: ${this.predicate}`;
  }
  toString() {
    return this.message;
  }
}
const Some = (promises, predicate) => new Promise((resolve, reject) => {
  let finishedCount = 0;
  const sharedState = {
    resolved: false
  };
  const errorArr = new Array(promises.length).fill(undefined);
  const resultArr = new Array(promises.length).fill(undefined);
  let predicateError;
  promises.forEach((x, index) => {
    x.then(resp => {
      resultArr[index] = resp;
      return undefined;
    }).catch(error => {
      errorArr[index] = error;
    })
    // eslint-disable-next-line promise/no-return-in-finally
    .finally(() => {
      if (sharedState.resolved) return;
      return predicate(resultArr.slice(0), sharedState).then(data => {
        sharedState.resolved = true;
        resolve(data);
        return undefined;
      }).catch(error => {
        // log only the last predicate error
        predicateError = error;
      }).finally(() => {
        finishedCount += 1;
        if (finishedCount === promises.length) {
          const errors = Object.values(resultArr.reduce((acc, z) => {
            if (z) {
              var _error$data;
              const {
                id,
                error
              } = z;
              if ((error === null || error === void 0 || (_error$data = error.data) === null || _error$data === void 0 ? void 0 : _error$data.length) > 0) {
                if (error.data.startsWith("Error occurred while verifying params")) acc[id] = capitalizeFirstLetter(error.data);else acc[id] = error.data;
              }
            }
            return acc;
          }, {}));
          if (errors.length > 0) {
            // Format-able errors
            const msg = errors.length > 1 ? `\n${errors.map(it => `â€¢ ${it}`).join("\n")}` : errors[0];
            reject(new Error(msg));
          } else {
            var _predicateError;
            reject(new SomeError({
              errors: errorArr,
              responses: resultArr,
              predicate: ((_predicateError = predicateError) === null || _predicateError === void 0 ? void 0 : _predicateError.message) || predicateError
            }));
          }
        }
      });
    });
  });
});

const GetPubKeyOrKeyAssign = async params => {
  const {
    endpoints,
    network,
    verifier,
    verifierId,
    extendedVerifierId,
    keyType
  } = params;
  const minThreshold = ~~(endpoints.length / 2) + 1;
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject(JRPC_METHODS.GET_OR_SET_KEY, {
    distributed_metadata: true,
    verifier,
    verifier_id: verifierId.toString(),
    extended_verifier_id: extendedVerifierId,
    one_key_flow: true,
    key_type: keyType,
    fetch_node_index: true,
    client_time: Math.floor(Date.now() / 1000).toString()
  }), {}, {
    logTracingHeader: config.logRequestTracing
  }).catch(err => log.error(`${JRPC_METHODS.GET_OR_SET_KEY} request failed`, err)));
  let nonceResult;
  const nodeIndexes = [];
  const result = await Some(lookupPromises, async lookupResults => {
    const lookupPubKeys = lookupResults.filter(x1 => {
      if (x1 && !x1.error) {
        return x1;
      }
      return false;
    });
    const errorResult = thresholdSame(lookupResults.map(x2 => x2 && x2.error), minThreshold);
    const keyResult = thresholdSame(lookupPubKeys.map(x3 => x3 && normalizeKeysResult(x3.result)), minThreshold);

    // check for nonce result in response if not a extendedVerifierId and not a legacy network
    if (keyResult && !nonceResult && !extendedVerifierId && !LEGACY_NETWORKS_ROUTE_MAP[network]) {
      for (let i = 0; i < lookupResults.length; i++) {
        const x1 = lookupResults[i];
        if (x1 && !x1.error) {
          var _x1$result;
          const currentNodePubKey = x1.result.keys[0].pub_key_X.toLowerCase();
          const thresholdPubKey = keyResult.keys[0].pub_key_X.toLowerCase();
          const pubNonceX = (_x1$result = x1.result) === null || _x1$result === void 0 || (_x1$result = _x1$result.keys[0].nonce_data) === null || _x1$result === void 0 || (_x1$result = _x1$result.pubNonce) === null || _x1$result === void 0 ? void 0 : _x1$result.x;
          if (pubNonceX && currentNodePubKey === thresholdPubKey) {
            nonceResult = x1.result.keys[0].nonce_data;
            break;
          }
        }
      }

      // if nonce result is not returned by nodes, fetch directly from metadata
      if (!nonceResult) {
        const metadataNonceResult = await getOrSetSapphireMetadataNonce(network, keyResult.keys[0].pub_key_X, keyResult.keys[0].pub_key_Y);
        // rechecking nonceResult to avoid promise race condition.
        if (!nonceResult && metadataNonceResult) {
          nonceResult = metadataNonceResult;
          if (nonceResult.nonce) {
            delete nonceResult.nonce;
          }
        }
      }
    }
    const serverTimeOffsets = [];
    // nonceResult must exist except for extendedVerifierId and legacy networks along with keyResult
    if (keyResult && (nonceResult || extendedVerifierId || LEGACY_NETWORKS_ROUTE_MAP[network]) || errorResult) {
      if (keyResult) {
        lookupResults.forEach(x1 => {
          if (x1 && x1.result) {
            const currentNodePubKey = x1.result.keys[0].pub_key_X.toLowerCase();
            const thresholdPubKey = keyResult.keys[0].pub_key_X.toLowerCase();
            // push only those indexes for nodes who are returning pub key matching with threshold pub key.
            // this check is important when different nodes have different keys assigned to a user.
            if (currentNodePubKey === thresholdPubKey) {
              const nodeIndex = Number.parseInt(x1.result.node_index);
              if (nodeIndex) nodeIndexes.push(nodeIndex);
            }
            const serverTimeOffset = x1.result.server_time_offset ? Number.parseInt(x1.result.server_time_offset, 10) : 0;
            serverTimeOffsets.push(serverTimeOffset);
          }
        });
      }
      const serverTimeOffset = keyResult ? calculateMedian(serverTimeOffsets) : 0;
      return Promise.resolve({
        keyResult,
        serverTimeOffset,
        nodeIndexes,
        errorResult,
        nonceResult
      });
    }
    return Promise.reject(new Error(`invalid public key result: ${JSON.stringify(lookupResults)} and nonce result:${JSON.stringify(nonceResult || {})} for verifier: ${verifier}, verifierId: ${verifierId} and extendedVerifierId: ${extendedVerifierId} `));
  });
  return result;
};
const VerifierLookupRequest = async params => {
  const {
    endpoints,
    verifier,
    verifierId,
    keyType
  } = params;
  const minThreshold = ~~(endpoints.length / 2) + 1;
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject(JRPC_METHODS.VERIFIER_LOOKUP, {
    verifier,
    verifier_id: verifierId.toString(),
    key_type: keyType,
    client_time: Math.floor(Date.now() / 1000).toString()
  }), {}, {
    logTracingHeader: config.logRequestTracing
  }).catch(err => log.error(`${JRPC_METHODS.GET_OR_SET_KEY} request failed`, err)));
  const result = await Some(lookupPromises, async lookupResults => {
    const lookupPubKeys = lookupResults.filter(x1 => {
      if (x1 && !x1.error) {
        return x1;
      }
      return false;
    });
    const errorResult = thresholdSame(lookupResults.map(x2 => x2 && x2.error), minThreshold);
    const keyResult = thresholdSame(lookupPubKeys.map(x3 => x3 && normalizeLookUpResult(x3.result)), minThreshold);
    const serverTimeOffsets = [];
    if (keyResult || errorResult) {
      const serverTimeOffset = keyResult ? calculateMedian(serverTimeOffsets) : 0;
      return Promise.resolve({
        keyResult,
        serverTimeOffset,
        errorResult
      });
    }
    return Promise.reject(new Error(`invalid lookup result: ${JSON.stringify(lookupResults)}
        )} for verifier: ${verifier}, verifierId: ${verifierId}`));
  });
  return result;
};
const commitmentRequest = async params => {
  const {
    idToken,
    endpoints,
    indexes,
    keyType,
    verifier,
    verifierParams,
    pubKeyX,
    pubKeyY,
    finalImportedShares,
    overrideExistingKey
  } = params;
  const tokenCommitment = keccak256(Buffer.from(idToken, "utf8"));
  const threeFourthsThreshold = ~~(endpoints.length * 3 / 4) + 1;
  const halfThreshold = ~~(endpoints.length / 2) + 1;
  const promiseArr = [];
  // make commitment requests to endpoints
  for (let i = 0; i < endpoints.length; i += 1) {
    /*
      CommitmentRequestParams struct {
        MessagePrefix      string `json:"messageprefix"`
        TokenCommitment    string `json:"tokencommitment"`
        TempPubX           string `json:"temppubx"`
        TempPubY           string `json:"temppuby"`
        VerifierIdentifier string `json:"verifieridentifier"`
      } 
      */
    const p = () => post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.COMMITMENT_REQUEST, {
      messageprefix: "mug00",
      keytype: keyType,
      tokencommitment: tokenCommitment.slice(2),
      temppubx: pubKeyX,
      temppuby: pubKeyY,
      verifieridentifier: verifier,
      verifier_id: verifierParams.verifier_id,
      extended_verifier_id: verifierParams.extended_verifier_id,
      is_import_key_flow: true
    }), {}, {
      logTracingHeader: config.logRequestTracing
    });
    const r = retryCommitment(p, 4);
    promiseArr.push(r);
  }
  return new Promise((resolve, reject) => {
    // send share request once k + t number of commitment requests have completed
    Some(promiseArr, resultArr => {
      const completedRequests = resultArr.filter(x => {
        if (!x || typeof x !== "object") {
          return false;
        }
        if (x.error) {
          return false;
        }
        return true;
      });
      if (finalImportedShares.length > 0) {
        // this is a optimization is for imported keys
        // for new imported keys registration we need to wait for all nodes to agree on commitment
        // for fetching existing imported keys we can rely on threshold nodes commitment
        if (overrideExistingKey && completedRequests.length === endpoints.length) {
          const requiredNodeResult = completedRequests.find(resp => {
            if (resp) {
              return true;
            }
            return false;
          });
          if (requiredNodeResult) {
            return Promise.resolve(resultArr);
          }
        } else if (!overrideExistingKey && completedRequests.length >= threeFourthsThreshold) {
          const nodeSigs = [];
          for (let i = 0; i < completedRequests.length; i += 1) {
            const x = completedRequests[i];
            if (!x || typeof x !== "object" || x.error) {
              continue;
            }
            if (x) nodeSigs.push(x.result);
          }
          const existingPubKey = thresholdSame(nodeSigs.map(x => x && x.pub_key_x), halfThreshold);
          const proxyEndpointNum = getProxyCoordinatorEndpointIndex(endpoints, verifier, verifierParams.verifier_id);
          // for import shares, proxy node response is required.
          // proxy node returns metadata.
          // if user's account already
          const requiredNodeIndex = indexes[proxyEndpointNum].toString(10);

          // if not a existing key we need to wait for nodes to agree on commitment
          if (existingPubKey || !existingPubKey && completedRequests.length === endpoints.length) {
            const requiredNodeResult = completedRequests.find(resp => {
              var _resp$result;
              if (resp && ((_resp$result = resp.result) === null || _resp$result === void 0 ? void 0 : _resp$result.nodeindex) === requiredNodeIndex) {
                return true;
              }
              return false;
            });
            if (requiredNodeResult) {
              return Promise.resolve(resultArr);
            }
          }
        }
      } else if (completedRequests.length >= threeFourthsThreshold) {
        // this case is for dkg keys
        const requiredNodeResult = completedRequests.find(resp => {
          if (resp) {
            return true;
          }
          return false;
        });
        if (requiredNodeResult) {
          return Promise.resolve(resultArr);
        }
      }
      return Promise.reject(new Error(`invalid commitment results ${JSON.stringify(resultArr)}`));
    }).then(resultArr => {
      return resolve(resultArr);
    }).catch(reject);
  });
};
async function retrieveOrImportShare(params) {
  const {
    legacyMetadataHost,
    enableOneKey,
    ecCurve,
    keyType,
    allowHost,
    network,
    clientId,
    endpoints,
    nodePubkeys,
    indexes,
    verifier,
    verifierParams,
    idToken,
    overrideExistingKey,
    newImportedShares,
    extraParams,
    useDkg = true,
    serverTimeOffset,
    checkCommitment = true
  } = params;
  await get(allowHost, {
    headers: {
      verifier,
      verifierid: verifierParams.verifier_id,
      network,
      clientid: clientId,
      enablegating: "true"
    }
  }, {
    useAPIKey: true
  });

  // generate temporary private and public key that is used to secure receive shares
  const sessionAuthKey = generatePrivate();
  const pubKey = getPublic(sessionAuthKey).toString("hex");
  const sessionPubX = pubKey.slice(2, 66);
  const sessionPubY = pubKey.slice(66);
  let finalImportedShares = [];
  const halfThreshold = ~~(endpoints.length / 2) + 1;
  if ((newImportedShares === null || newImportedShares === void 0 ? void 0 : newImportedShares.length) > 0) {
    if (newImportedShares.length !== endpoints.length) {
      throw new Error("Invalid imported shares length");
    }
    finalImportedShares = newImportedShares;
  } else if (!useDkg) {
    const bufferKey = keyType === KEY_TYPE.SECP256K1 ? generatePrivateKey(ecCurve, Buffer) : await getRandomBytes(32);
    const generatedShares = await generateShares(ecCurve, keyType, serverTimeOffset, indexes, nodePubkeys, Buffer.from(bufferKey));
    finalImportedShares = [...finalImportedShares, ...generatedShares];
  }
  let commitmentRequestResult = [];
  let isExistingKey;
  const nodeSigs = [];
  if (checkCommitment) {
    commitmentRequestResult = await commitmentRequest({
      idToken,
      endpoints,
      indexes,
      keyType,
      verifier,
      verifierParams,
      pubKeyX: sessionPubX,
      pubKeyY: sessionPubY,
      finalImportedShares,
      overrideExistingKey
    });
    for (let i = 0; i < commitmentRequestResult.length; i += 1) {
      const x = commitmentRequestResult[i];
      if (!x || typeof x !== "object" || x.error) {
        continue;
      }
      if (x) nodeSigs.push(x.result);
    }
    // if user's account already
    isExistingKey = !!thresholdSame(nodeSigs.map(x => x && x.pub_key_x), halfThreshold);
  } else if (!checkCommitment && finalImportedShares.length > 0) {
    // in case not allowed to override existing key for import request
    // check if key exists
    if (!overrideExistingKey) {
      var _keyLookupResult$erro, _keyLookupResult$keyR;
      const keyLookupResult = await VerifierLookupRequest({
        endpoints,
        verifier,
        verifierId: verifierParams.verifier_id,
        keyType
      });
      if (keyLookupResult.errorResult && !((_keyLookupResult$erro = keyLookupResult.errorResult) !== null && _keyLookupResult$erro !== void 0 && (_keyLookupResult$erro = _keyLookupResult$erro.data) !== null && _keyLookupResult$erro !== void 0 && _keyLookupResult$erro.includes("Verifier + VerifierID has not yet been assigned"))) {
        throw new Error(`node results do not match at first lookup ${JSON.stringify(keyLookupResult.keyResult || {})}, ${JSON.stringify(keyLookupResult.errorResult || {})}`);
      }
      if (((_keyLookupResult$keyR = keyLookupResult.keyResult) === null || _keyLookupResult$keyR === void 0 || (_keyLookupResult$keyR = _keyLookupResult$keyR.keys) === null || _keyLookupResult$keyR === void 0 ? void 0 : _keyLookupResult$keyR.length) > 0) {
        isExistingKey = !!keyLookupResult.keyResult.keys[0];
      }
    }
  }
  const promiseArrRequest = [];
  const canImportedShares = overrideExistingKey || !useDkg && !isExistingKey;
  if (canImportedShares) {
    const proxyEndpointNum = getProxyCoordinatorEndpointIndex(endpoints, verifier, verifierParams.verifier_id);
    const items = [];
    for (let i = 0; i < endpoints.length; i += 1) {
      const importedShare = finalImportedShares[i];
      if (!importedShare) {
        throw new Error(`invalid imported share at index ${i}`);
      }
      items.push(_objectSpread(_objectSpread({}, verifierParams), {}, {
        idtoken: idToken,
        nodesignatures: nodeSigs,
        verifieridentifier: verifier,
        pub_key_x: importedShare.oauth_pub_key_x,
        pub_key_y: importedShare.oauth_pub_key_y,
        signing_pub_key_x: importedShare.signing_pub_key_x,
        signing_pub_key_y: importedShare.signing_pub_key_y,
        encrypted_share: importedShare.encrypted_share,
        encrypted_share_metadata: importedShare.encrypted_share_metadata,
        node_index: importedShare.node_index,
        key_type: importedShare.key_type,
        nonce_data: importedShare.nonce_data,
        nonce_signature: importedShare.nonce_signature,
        sss_endpoint: endpoints[i]
      }, extraParams));
    }
    const p = post(endpoints[proxyEndpointNum], generateJsonRPCObject(JRPC_METHODS.IMPORT_SHARES, {
      encrypted: "yes",
      use_temp: true,
      verifieridentifier: verifier,
      temppubx: nodeSigs.length === 0 && !checkCommitment ? sessionPubX : "",
      // send session pub key x only if node signatures are not available (Ie. in non commitment flow)
      temppuby: nodeSigs.length === 0 && !checkCommitment ? sessionPubY : "",
      // send session pub key y only if node signatures are not available (Ie. in non commitment flow)
      item: items,
      key_type: keyType,
      one_key_flow: true
    }), {}, {
      logTracingHeader: config.logRequestTracing
    }).catch(err => log.error("share req", err));
    promiseArrRequest.push(p);
  } else {
    for (let i = 0; i < endpoints.length; i += 1) {
      const p = post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.GET_SHARE_OR_KEY_ASSIGN, {
        encrypted: "yes",
        use_temp: true,
        key_type: keyType,
        distributed_metadata: true,
        verifieridentifier: verifier,
        temppubx: nodeSigs.length === 0 && !checkCommitment ? sessionPubX : "",
        // send session pub key x only if node signatures are not available (Ie. in non commitment flow)
        temppuby: nodeSigs.length === 0 && !checkCommitment ? sessionPubY : "",
        // send session pub key y only if node signatures are not available (Ie. in non commitment flow)
        item: [_objectSpread(_objectSpread({}, verifierParams), {}, {
          idtoken: idToken,
          key_type: keyType,
          nodesignatures: nodeSigs,
          verifieridentifier: verifier
        }, extraParams)],
        client_time: Math.floor(Date.now() / 1000).toString(),
        one_key_flow: true
      }), {}, {
        logTracingHeader: config.logRequestTracing
      });
      promiseArrRequest.push(p);
    }
  }
  return Some(promiseArrRequest, async (shareResponseResult, sharedState) => {
    let thresholdNonceData;
    let shareResponses = [];
    // for import shares case, where result is an array
    if (shareResponseResult.length === 1 && shareResponseResult[0] && Array.isArray(shareResponseResult[0].result)) {
      // this is for import shares
      const importedSharesResult = shareResponseResult[0];
      shareResponseResult[0].result.forEach(res => {
        shareResponses.push({
          id: importedSharesResult.id,
          jsonrpc: "2.0",
          result: res,
          error: importedSharesResult.error
        });
      });
    } else {
      shareResponses = shareResponseResult;
    }
    // check if threshold number of nodes have returned the same user public key
    const completedRequests = shareResponses.filter(x => {
      if (!x || typeof x !== "object") {
        return false;
      }
      if (x.error) {
        return false;
      }
      return true;
    });
    const pubkeys = shareResponses.map(x => {
      if (x && x.result && x.result.keys[0].public_key) {
        return x.result.keys[0].public_key;
      }
      return undefined;
    });
    const thresholdPublicKey = thresholdSame(pubkeys, halfThreshold);
    if (!thresholdPublicKey) {
      throw new Error("invalid result from nodes, threshold number of public key results are not matching");
    }
    shareResponses.forEach(x => {
      const requiredShareResponse = x && x.result && x.result.keys[0].public_key && x.result.keys[0];
      if (requiredShareResponse && !thresholdNonceData && !verifierParams.extended_verifier_id) {
        var _requiredShareRespons;
        const currentPubKey = requiredShareResponse.public_key;
        const pubNonce = (_requiredShareRespons = requiredShareResponse.nonce_data) === null || _requiredShareRespons === void 0 || (_requiredShareRespons = _requiredShareRespons.pubNonce) === null || _requiredShareRespons === void 0 ? void 0 : _requiredShareRespons.x;
        if (pubNonce && currentPubKey.X === thresholdPublicKey.X) {
          thresholdNonceData = requiredShareResponse.nonce_data;
        }
      }
    });
    const thresholdReqCount = canImportedShares ? endpoints.length : halfThreshold;
    // optimistically run lagrange interpolation once threshold number of shares have been received
    // this is matched against the user public key to ensure that shares are consistent
    // Note: no need of thresholdMetadataNonce for extended_verifier_id key
    if (completedRequests.length >= thresholdReqCount && thresholdPublicKey) {
      const sharePromises = [];
      const sessionTokenSigPromises = [];
      const sessionTokenPromises = [];
      const nodeIndexes = [];
      const sessionTokenData = [];
      const isNewKeyResponses = [];
      const serverTimeOffsetResponses = [];
      for (let i = 0; i < completedRequests.length; i += 1) {
        var _currentShareResponse;
        const currentShareResponse = completedRequests[i];
        const {
          session_tokens: sessionTokens,
          session_token_metadata: sessionTokenMetadata,
          session_token_sigs: sessionTokenSigs,
          session_token_sig_metadata: sessionTokenSigMetadata,
          keys,
          is_new_key: isNewKey,
          server_time_offset: serverTimeOffsetResponse
        } = currentShareResponse.result;
        isNewKeyResponses.push({
          isNewKey,
          publicKey: ((_currentShareResponse = currentShareResponse.result) === null || _currentShareResponse === void 0 || (_currentShareResponse = _currentShareResponse.keys[0]) === null || _currentShareResponse === void 0 || (_currentShareResponse = _currentShareResponse.public_key) === null || _currentShareResponse === void 0 ? void 0 : _currentShareResponse.X) || ""
        });
        serverTimeOffsetResponses.push(serverTimeOffsetResponse || "0");
        if ((sessionTokenSigs === null || sessionTokenSigs === void 0 ? void 0 : sessionTokenSigs.length) > 0) {
          var _sessionTokenSigMetad;
          // decrypt sessionSig if enc metadata is sent
          if (sessionTokenSigMetadata && (_sessionTokenSigMetad = sessionTokenSigMetadata[0]) !== null && _sessionTokenSigMetad !== void 0 && _sessionTokenSigMetad.ephemPublicKey) {
            sessionTokenSigPromises.push(decryptNodeData(sessionTokenSigMetadata[0], sessionTokenSigs[0], sessionAuthKey).catch(err => log.error("session sig decryption", err)));
          } else {
            sessionTokenSigPromises.push(Promise.resolve(Buffer.from(sessionTokenSigs[0], "hex")));
          }
        } else {
          sessionTokenSigPromises.push(Promise.resolve(undefined));
        }
        if ((sessionTokens === null || sessionTokens === void 0 ? void 0 : sessionTokens.length) > 0) {
          var _sessionTokenMetadata;
          // decrypt session token if enc metadata is sent
          if (sessionTokenMetadata && (_sessionTokenMetadata = sessionTokenMetadata[0]) !== null && _sessionTokenMetadata !== void 0 && _sessionTokenMetadata.ephemPublicKey) {
            sessionTokenPromises.push(decryptNodeData(sessionTokenMetadata[0], sessionTokens[0], sessionAuthKey).catch(err => log.error("session token sig decryption", err)));
          } else {
            sessionTokenPromises.push(Promise.resolve(Buffer.from(sessionTokens[0], "base64")));
          }
        } else {
          sessionTokenPromises.push(Promise.resolve(undefined));
        }
        if ((keys === null || keys === void 0 ? void 0 : keys.length) > 0) {
          const latestKey = currentShareResponse.result.keys[0];
          nodeIndexes.push(new BN$1(latestKey.node_index));
          if (latestKey.share_metadata) {
            sharePromises.push(decryptNodeDataWithPadding(latestKey.share_metadata, Buffer.from(latestKey.share, "base64").toString("binary"), sessionAuthKey).catch(err => log.error("share decryption", err)));
          }
        } else {
          nodeIndexes.push(undefined);
          sharePromises.push(Promise.resolve(undefined));
        }
      }
      const allPromises = await Promise.all(sharePromises.concat(sessionTokenSigPromises).concat(sessionTokenPromises));
      const sharesResolved = allPromises.slice(0, sharePromises.length);
      const sessionSigsResolved = allPromises.slice(sharePromises.length, sharePromises.length + sessionTokenSigPromises.length);
      const sessionTokensResolved = allPromises.slice(sharePromises.length + sessionTokenSigPromises.length, allPromises.length);
      const validSigs = sessionSigsResolved.filter(sig => {
        if (sig) {
          return true;
        }
        return false;
      });
      if (!verifierParams.extended_verifier_id && validSigs.length < halfThreshold) {
        throw new Error(`Insufficient number of signatures from nodes, required: ${halfThreshold}, found: ${validSigs.length}`);
      }
      const validTokens = sessionTokensResolved.filter(token => {
        if (token) {
          return true;
        }
        return false;
      });
      if (!verifierParams.extended_verifier_id && validTokens.length < halfThreshold) {
        throw new Error(`Insufficient number of session tokens from nodes, required: ${halfThreshold}, found: ${validTokens.length}`);
      }
      sessionTokensResolved.forEach((x, index) => {
        if (!x || !sessionSigsResolved[index]) sessionTokenData.push(undefined);else sessionTokenData.push({
          token: x.toString("base64"),
          signature: sessionSigsResolved[index].toString("hex"),
          node_pubx: completedRequests[index].result.node_pubx,
          node_puby: completedRequests[index].result.node_puby
        });
      });
      if (sharedState.resolved) return undefined;
      const decryptedShares = sharesResolved.reduce((acc, curr, index) => {
        if (curr) {
          acc.push({
            index: nodeIndexes[index],
            value: new BN$1(curr)
          });
        }
        return acc;
      }, []);
      // run lagrange interpolation on all subsets, faster in the optimistic scenario than berlekamp-welch due to early exit
      const allCombis = kCombinations(decryptedShares.length, halfThreshold);
      let privateKey = null;
      for (let j = 0; j < allCombis.length; j += 1) {
        const currentCombi = allCombis[j];
        const currentCombiShares = decryptedShares.filter((_, index) => currentCombi.includes(index));
        const shares = currentCombiShares.map(x => x.value);
        const indices = currentCombiShares.map(x => x.index);
        const derivedPrivateKey = lagrangeInterpolation(ecCurve, shares, indices);
        if (!derivedPrivateKey) continue;
        const decryptedPubKey = derivePubKey(ecCurve, derivedPrivateKey);
        const decryptedPubKeyX = decryptedPubKey.getX();
        const decryptedPubKeyY = decryptedPubKey.getY();
        if (decryptedPubKeyX.cmp(new BN$1(thresholdPublicKey.X, 16)) === 0 && decryptedPubKeyY.cmp(new BN$1(thresholdPublicKey.Y, 16)) === 0) {
          privateKey = derivedPrivateKey;
          break;
        }
      }
      if (privateKey === undefined || privateKey === null) {
        throw new Error("could not derive private key");
      }
      let isNewKey = false;
      isNewKeyResponses.forEach(x => {
        if (x.isNewKey === "true" && x.publicKey.toLowerCase() === thresholdPublicKey.X.toLowerCase()) {
          isNewKey = true;
        }
      });

      // Convert each string timestamp to a number
      const serverOffsetTimes = serverTimeOffsetResponses.map(timestamp => Number.parseInt(timestamp, 10));
      return {
        privateKey,
        sessionTokenData,
        thresholdNonceData,
        nodeIndexes,
        thresholdPubKey: thresholdPublicKey,
        isNewKey,
        serverTimeOffsetResponse: serverTimeOffset || calculateMedian(serverOffsetTimes)
      };
    }
    if (completedRequests.length < thresholdReqCount) {
      throw new Error(`Waiting for results from more nodes, pending: ${thresholdReqCount - completedRequests.length}`);
    }
    throw new Error(`Invalid results, threshold pub key: ${thresholdPublicKey}, nonce data found: ${!!thresholdNonceData}, extended verifierId: ${verifierParams.extended_verifier_id}`);
  }).then(async res => {
    var _nonceResult;
    const {
      privateKey,
      thresholdPubKey,
      sessionTokenData,
      nodeIndexes,
      thresholdNonceData,
      isNewKey,
      serverTimeOffsetResponse
    } = res;
    let nonceResult = thresholdNonceData;
    if (!privateKey) throw new Error("Invalid private key returned");
    const oAuthKey = privateKey;
    const oAuthPubKey = derivePubKey(ecCurve, oAuthKey);
    const oAuthPubkeyX = oAuthPubKey.getX().toString("hex", 64);
    const oAuthPubkeyY = oAuthPubKey.getY().toString("hex", 64);

    // if both thresholdNonceData and extended_verifier_id are not available
    // then we need to throw other wise address would be incorrect.
    if (!nonceResult && !verifierParams.extended_verifier_id && !LEGACY_NETWORKS_ROUTE_MAP[network]) {
      // NOTE: dont use padded pub key anywhere in metadata apis, send pub keys as is received from nodes.
      const metadataNonceResult = await getOrSetSapphireMetadataNonce(network, thresholdPubKey.X, thresholdPubKey.Y, serverTimeOffset, oAuthKey);
      // rechecking nonceResult to avoid promise race condition.
      if (metadataNonceResult && !thresholdNonceData) {
        nonceResult = metadataNonceResult;
      } else {
        throw new Error(`invalid metadata result from nodes, nonce metadata is empty for verifier: ${verifier} and verifierId: ${verifierParams.verifier_id}`);
      }
    }
    let metadataNonce = new BN$1((_nonceResult = nonceResult) !== null && _nonceResult !== void 0 && _nonceResult.nonce ? nonceResult.nonce.padStart(64, "0") : "0", "hex");
    let finalPubKey;
    let pubNonce;
    let typeOfUser = "v1";
    // extended_verifier_id is only exception for torus-test-health verifier
    // otherwise extended verifier id should not even return shares.
    if (verifierParams.extended_verifier_id) {
      typeOfUser = "v2";
      // for tss key no need to add pub nonce
      finalPubKey = ecCurve.keyFromPublic({
        x: oAuthPubkeyX,
        y: oAuthPubkeyY
      }).getPublic();
    } else if (LEGACY_NETWORKS_ROUTE_MAP[network]) {
      if (enableOneKey) {
        nonceResult = await getOrSetNonce(legacyMetadataHost, ecCurve, serverTimeOffsetResponse, oAuthPubkeyX, oAuthPubkeyY, oAuthKey, !isNewKey);
        metadataNonce = new BN$1(nonceResult.nonce || "0", 16);
        typeOfUser = nonceResult.typeOfUser;
        if (typeOfUser === "v2") {
          pubNonce = {
            X: nonceResult.pubNonce.x,
            Y: nonceResult.pubNonce.y
          };
          finalPubKey = ecCurve.keyFromPublic({
            x: oAuthPubkeyX,
            y: oAuthPubkeyY
          }).getPublic().add(ecCurve.keyFromPublic({
            x: nonceResult.pubNonce.x,
            y: nonceResult.pubNonce.y
          }).getPublic());
        } else {
          typeOfUser = "v1";
          // for imported keys in legacy networks
          metadataNonce = await getMetadata(legacyMetadataHost, {
            pub_key_X: oAuthPubkeyX,
            pub_key_Y: oAuthPubkeyY
          });
          const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.n);
          finalPubKey = ecCurve.keyFromPrivate(privateKeyWithNonce.toString(16, 64), "hex").getPublic();
        }
      } else {
        typeOfUser = "v1";
        // for imported keys in legacy networks
        metadataNonce = await getMetadata(legacyMetadataHost, {
          pub_key_X: oAuthPubkeyX,
          pub_key_Y: oAuthPubkeyY
        });
        const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.n);
        finalPubKey = ecCurve.keyFromPrivate(privateKeyWithNonce.toString(16, 64), "hex").getPublic();
      }
    } else {
      typeOfUser = "v2";
      finalPubKey = ecCurve.keyFromPublic({
        x: oAuthPubkeyX,
        y: oAuthPubkeyY
      }).getPublic().add(ecCurve.keyFromPublic({
        x: nonceResult.pubNonce.x,
        y: nonceResult.pubNonce.y
      }).getPublic());
      pubNonce = {
        X: nonceResult.pubNonce.x,
        Y: nonceResult.pubNonce.y
      };
    }
    if (!finalPubKey) {
      throw new Error("Invalid public key, this might be a bug, please report this to web3auth team");
    }
    let finalPrivKey = ""; // it is empty for v2 user upgraded to 2/n
    let isUpgraded = false;
    const oAuthKeyAddress = generateAddressFromPrivKey(keyType, oAuthKey);
    // deriving address from pub key coz pubkey is always available
    // but finalPrivKey won't be available for  v2 user upgraded to 2/n
    const finalWalletAddress = generateAddressFromPubKey(keyType, finalPubKey.getX(), finalPubKey.getY());
    let keyWithNonce = "";
    if (typeOfUser === "v1") {
      isUpgraded = null;
    } else if (typeOfUser === "v2") {
      isUpgraded = metadataNonce.eq(new BN$1("0"));
    }
    if (typeOfUser === "v1" || typeOfUser === "v2" && metadataNonce.gt(new BN$1(0))) {
      const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.n);
      keyWithNonce = privateKeyWithNonce.toString("hex", 64);
    }
    if (keyType === KEY_TYPE.SECP256K1) {
      finalPrivKey = keyWithNonce;
    } else if (keyType === KEY_TYPE.ED25519) {
      if (keyWithNonce && !nonceResult.seed) {
        throw new Error("Invalid data, seed data is missing for ed25519 key, Please report this bug");
      } else if (keyWithNonce && nonceResult.seed) {
        // console.log("nonceResult.seed", nonceResult.seed, keyWithNonce);
        const decryptedSeed = await decryptSeedData(nonceResult.seed, new BN$1(keyWithNonce, "hex"));
        finalPrivKey = decryptedSeed.toString("hex");
      }
    } else {
      throw new Error(`Invalid keyType: ${keyType}`);
    }
    let postboxKey = oAuthKey;
    let postboxPubX = oAuthPubkeyX;
    let postboxPubY = oAuthPubkeyY;
    if (keyType === KEY_TYPE.ED25519) {
      const {
        scalar,
        point
      } = getSecpKeyFromEd25519(privateKey);
      postboxKey = scalar;
      postboxPubX = point.getX().toString(16, 64);
      postboxPubY = point.getY().toString(16, 64);
      if (thresholdPubKey.SignerX.padStart(64, "0") !== postboxPubX || thresholdPubKey.SignerY.padStart(64, "0") !== postboxPubY) {
        throw new Error("Invalid postbox key");
      }
    }
    // return reconstructed private key and ethereum address
    return {
      finalKeyData: {
        walletAddress: finalWalletAddress,
        X: finalPubKey.getX().toString(16, 64),
        // this is final pub x user before and after updating to 2/n
        Y: finalPubKey.getY().toString(16, 64),
        // this is final pub y user before and after updating to 2/n
        privKey: finalPrivKey
      },
      oAuthKeyData: {
        walletAddress: oAuthKeyAddress,
        X: oAuthPubkeyX,
        Y: oAuthPubkeyY,
        privKey: oAuthKey.toString("hex", 64)
      },
      postboxKeyData: {
        privKey: postboxKey.toString("hex", 64),
        X: postboxPubX,
        Y: postboxPubY
      },
      sessionData: {
        sessionTokenData,
        sessionAuthKey: sessionAuthKey.toString("hex").padStart(64, "0")
      },
      metadata: {
        pubNonce,
        nonce: metadataNonce,
        typeOfUser,
        upgraded: isUpgraded,
        serverTimeOffset: serverTimeOffsetResponse
      },
      nodesData: {
        nodeIndexes: nodeIndexes.map(x => x.toNumber())
      }
    };
  });
}

// Note: Endpoints should be the sss node endpoints along with path

// for ex: [https://node-1.node.web3auth.io/sss/jrpc, https://node-2.node.web3auth.io/sss/jrpc ....]
const GetOrSetTssDKGPubKey = async params => {
  const {
    endpoints,
    verifier,
    verifierId,
    tssVerifierId,
    keyType = KEY_TYPE.SECP256K1
  } = params;
  const minThreshold = ~~(endpoints.length / 2) + 1;
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject(JRPC_METHODS.GET_OR_SET_KEY, {
    distributed_metadata: true,
    verifier,
    verifier_id: verifierId,
    extended_verifier_id: tssVerifierId,
    one_key_flow: true,
    key_type: keyType,
    fetch_node_index: true,
    client_time: Math.floor(Date.now() / 1000).toString()
  }), {}, {
    logTracingHeader: false
  }).catch(err => log$1.error(`${JRPC_METHODS.GET_OR_SET_KEY} request failed`, err)));
  const nodeIndexes = [];
  const result = await Some(lookupPromises, async lookupResults => {
    const lookupPubKeys = lookupResults.filter(x1 => {
      if (x1 && !x1.error) {
        return x1;
      }
      return false;
    });
    const errorResult = thresholdSame(lookupResults.map(x2 => x2 && x2.error), minThreshold);
    const keyResult = thresholdSame(lookupPubKeys.map(x3 => x3 && normalizeKeysResult(x3.result)), minThreshold);
    if (keyResult || errorResult) {
      if (keyResult) {
        lookupResults.forEach(x1 => {
          if (x1 && x1.result) {
            const currentNodePubKey = x1.result.keys[0].pub_key_X.toLowerCase();
            const thresholdPubKey = keyResult.keys[0].pub_key_X.toLowerCase();
            // push only those indexes for nodes who are returning pub key matching with threshold pub key.
            // this check is important when different nodes have different keys assigned to a user.
            if (currentNodePubKey === thresholdPubKey) {
              const nodeIndex = Number.parseInt(x1.result.node_index);
              if (nodeIndex) nodeIndexes.push(nodeIndex);
            }
          }
        });
      }
      return Promise.resolve({
        keyResult,
        nodeIndexes,
        errorResult
      });
    }
    return Promise.reject(new Error(`invalid public key result: ${JSON.stringify(lookupResults)} for tssVerifierId: ${tssVerifierId} `));
  });
  if (result.errorResult) {
    throw new Error(`invalid public key result,errorResult: ${JSON.stringify(result.errorResult)}`);
  }
  const key = result.keyResult.keys[0];
  return {
    key: {
      pubKeyX: key.pub_key_X,
      pubKeyY: key.pub_key_Y,
      address: key.address,
      createdAt: key.created_at
    },
    nodeIndexes: result.nodeIndexes,
    isNewKey: result.keyResult.is_new_key
  };
};

// Implement threshold logic wrappers around public APIs
// of Torus nodes to handle malicious node responses
class Torus {
  constructor({
    enableOneKey = false,
    clientId,
    network,
    serverTimeOffset = 0,
    allowHost,
    legacyMetadataHost,
    keyType = KEY_TYPE.SECP256K1
  }) {
    // 86400 = 24 hour
    _defineProperty(this, "allowHost", void 0);
    _defineProperty(this, "serverTimeOffset", void 0);
    _defineProperty(this, "network", void 0);
    _defineProperty(this, "clientId", void 0);
    _defineProperty(this, "ec", void 0);
    _defineProperty(this, "enableOneKey", void 0);
    _defineProperty(this, "legacyMetadataHost", void 0);
    _defineProperty(this, "keyType", KEY_TYPE.SECP256K1);
    if (!clientId) throw new Error("Please provide a valid clientId in constructor");
    if (!network) throw new Error("Please provide a valid network in constructor");
    if (keyType === KEY_TYPE.ED25519 && LEGACY_NETWORKS_ROUTE_MAP[network]) {
      throw new Error(`keyType: ${keyType} is not supported by ${network} network`);
    }
    this.keyType = keyType;
    this.ec = new ec(this.keyType);
    this.serverTimeOffset = serverTimeOffset || 0; // ms
    this.network = network;
    this.clientId = clientId;
    this.allowHost = allowHost || `${SIGNER_MAP[network]}/api/allow`;
    this.enableOneKey = enableOneKey;
    this.legacyMetadataHost = legacyMetadataHost || METADATA_MAP[network];
  }
  static enableLogging(v = true) {
    if (v) {
      log.enableAll();
      config.logRequestTracing = true;
    } else log.disableAll();
  }
  static setAPIKey(apiKey) {
    setAPIKey(apiKey);
  }
  static setEmbedHost(embedHost) {
    setEmbedHost(embedHost);
  }
  static setSessionTime(sessionTime) {
    Torus.sessionTime = sessionTime;
  }
  static isGetOrSetNonceError(err) {
    return err instanceof GetOrSetNonceError;
  }
  static getPostboxKey(torusKey) {
    if (torusKey.metadata.typeOfUser === "v1") {
      return torusKey.finalKeyData.privKey || torusKey.postboxKeyData.privKey;
    }
    return torusKey.postboxKeyData.privKey;
  }
  async retrieveShares(params) {
    const {
      verifier,
      verifierParams,
      idToken,
      nodePubkeys,
      indexes,
      endpoints,
      useDkg,
      extraParams = {},
      checkCommitment = true
    } = params;
    if (nodePubkeys.length === 0) {
      throw new Error("nodePubkeys param is required");
    }
    if (nodePubkeys.length !== indexes.length) {
      throw new Error("nodePubkeys length must be same as indexes length");
    }
    if (nodePubkeys.length !== endpoints.length) {
      throw new Error("nodePubkeys length must be same as endpoints length");
    }
    // dkg is used by default only for secp256k1 keys,
    // for ed25519 keys import keys flows is the default
    let shouldUseDkg;
    if (typeof useDkg === "boolean") {
      if (useDkg === false && LEGACY_NETWORKS_ROUTE_MAP[this.network]) {
        throw new Error(`useDkg cannot be false for legacy network; ${this.network}`);
      }
      shouldUseDkg = this.keyType === KEY_TYPE.ED25519 ? false : useDkg;
    } else if (this.keyType === KEY_TYPE.ED25519) {
      shouldUseDkg = false;
    } else {
      shouldUseDkg = true;
    }
    if (!shouldUseDkg && nodePubkeys.length === 0) {
      throw new Error("nodePubkeys param is required");
    }
    if (!extraParams.session_token_exp_second) {
      extraParams.session_token_exp_second = Torus.sessionTime;
    }
    return retrieveOrImportShare({
      legacyMetadataHost: this.legacyMetadataHost,
      serverTimeOffset: this.serverTimeOffset,
      enableOneKey: this.enableOneKey,
      ecCurve: this.ec,
      keyType: this.keyType,
      allowHost: this.allowHost,
      network: this.network,
      clientId: this.clientId,
      endpoints,
      indexes,
      verifier,
      verifierParams,
      idToken,
      useDkg: shouldUseDkg,
      newImportedShares: [],
      overrideExistingKey: false,
      nodePubkeys,
      extraParams,
      checkCommitment
    });
  }
  async getPublicAddress(endpoints, torusNodePubs, {
    verifier,
    verifierId,
    extendedVerifierId
  }) {
    log.info(torusNodePubs, {
      verifier,
      verifierId,
      extendedVerifierId
    });
    return this.getNewPublicAddress(endpoints, {
      verifier,
      verifierId,
      extendedVerifierId
    }, this.enableOneKey);
  }
  async importPrivateKey(params) {
    const {
      nodeIndexes,
      newPrivateKey,
      verifier,
      verifierParams,
      idToken,
      nodePubkeys,
      endpoints,
      extraParams = {},
      checkCommitment = true
    } = params;
    if (LEGACY_NETWORKS_ROUTE_MAP[this.network]) {
      throw new Error(`importPrivateKey is not supported by legacy network; ${this.network}`);
    }
    if (endpoints.length !== nodeIndexes.length) {
      throw new Error(`length of endpoints array must be same as length of nodeIndexes array`);
    }
    if (!extraParams.session_token_exp_second) {
      extraParams.session_token_exp_second = Torus.sessionTime;
    }
    let privKeyBuffer;
    if (this.keyType === KEY_TYPE.SECP256K1) {
      privKeyBuffer = Buffer.from(newPrivateKey.padStart(64, "0"), "hex");
      if (privKeyBuffer.length !== 32) {
        throw new Error("Invalid private key length for given secp256k1 key");
      }
    }
    if (this.keyType === KEY_TYPE.ED25519) {
      privKeyBuffer = Buffer.from(newPrivateKey.padStart(64, "0"), "hex");
      if (privKeyBuffer.length !== 32) {
        throw new Error("Invalid private key length for given ed25519 key");
      }
    }
    const sharesData = await generateShares(this.ec, this.keyType, this.serverTimeOffset, nodeIndexes, nodePubkeys, privKeyBuffer);
    if (this.keyType === KEY_TYPE.ED25519) {
      const ed25519Key = getEd25519ExtendedPublicKey(privKeyBuffer);
      const ed25519PubKey = encodeEd25519Point(ed25519Key.point);
      const encodedPubKey = encodeEd25519Point(sharesData[0].final_user_point);
      const importedPubKey = Buffer.from(ed25519PubKey).toString("hex");
      const derivedPubKey = encodedPubKey.toString("hex");
      if (importedPubKey !== derivedPubKey) {
        throw new Error("invalid shares data for ed25519 key, public key is not matching after generating shares");
      }
    }
    return retrieveOrImportShare({
      legacyMetadataHost: this.legacyMetadataHost,
      serverTimeOffset: this.serverTimeOffset,
      enableOneKey: this.enableOneKey,
      ecCurve: this.ec,
      keyType: this.keyType,
      allowHost: this.allowHost,
      network: this.network,
      clientId: this.clientId,
      endpoints,
      indexes: nodeIndexes,
      verifier,
      verifierParams,
      idToken,
      useDkg: false,
      overrideExistingKey: true,
      newImportedShares: sharesData,
      nodePubkeys,
      extraParams,
      checkCommitment
    });
  }

  /**
   * Note: use this function only for openlogin tkey account lookups.
   * this is a legacy function, use getPublicAddress instead for new networks
   */
  async getUserTypeAndAddress(endpoints, {
    verifier,
    verifierId,
    extendedVerifierId
  }) {
    return this.getNewPublicAddress(endpoints, {
      verifier,
      verifierId,
      extendedVerifierId
    }, true);
  }
  async getNewPublicAddress(endpoints, {
    verifier,
    verifierId,
    extendedVerifierId
  }, enableOneKey) {
    const keyAssignResult = await GetPubKeyOrKeyAssign({
      endpoints,
      network: this.network,
      verifier,
      verifierId,
      keyType: this.keyType,
      extendedVerifierId
    });
    const {
      errorResult,
      keyResult,
      nodeIndexes = [],
      serverTimeOffset
    } = keyAssignResult;
    const finalServerTimeOffset = this.serverTimeOffset || serverTimeOffset;
    const {
      nonceResult
    } = keyAssignResult;
    if (errorResult && JSON.stringify(errorResult).toLowerCase().includes("verifier not supported")) {
      // change error msg
      throw new Error(`Verifier not supported. Check if you: \n
      1. Are on the right network (Torus testnet/mainnet) \n
      2. Have setup a verifier on dashboard.web3auth.io?`);
    }
    if (errorResult) {
      throw new Error(`node results do not match at first lookup ${JSON.stringify(keyResult || {})}, ${JSON.stringify(errorResult || {})}`);
    }
    if (!(keyResult !== null && keyResult !== void 0 && keyResult.keys)) {
      throw new Error(`node results do not match at final lookup ${JSON.stringify(keyResult || {})}, ${JSON.stringify(errorResult || {})}`);
    }

    // no need of nonce for extendedVerifierId (tss verifier id)
    if (!nonceResult && !extendedVerifierId && !LEGACY_NETWORKS_ROUTE_MAP[this.network]) {
      throw new GetOrSetNonceError("metadata nonce is missing in share response");
    }
    const {
      pub_key_X: X,
      pub_key_Y: Y
    } = keyResult.keys[0];
    let pubNonce;
    const nonce = new BN$1((nonceResult === null || nonceResult === void 0 ? void 0 : nonceResult.nonce) || "0", 16);
    let oAuthPubKey;
    let finalPubKey;
    if (extendedVerifierId) {
      // for tss key no need to add pub nonce
      finalPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic();
      oAuthPubKey = finalPubKey;
    } else if (LEGACY_NETWORKS_ROUTE_MAP[this.network]) {
      return this.formatLegacyPublicKeyData({
        isNewKey: keyResult.is_new_key,
        enableOneKey,
        finalKeyResult: {
          keys: keyResult.keys
        },
        serverTimeOffset: finalServerTimeOffset
      });
    } else {
      const v2NonceResult = nonceResult;
      oAuthPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic();
      finalPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic().add(this.ec.keyFromPublic({
        x: v2NonceResult.pubNonce.x,
        y: v2NonceResult.pubNonce.y
      }).getPublic());
      pubNonce = {
        X: v2NonceResult.pubNonce.x,
        Y: v2NonceResult.pubNonce.y
      };
    }
    if (!oAuthPubKey) {
      throw new Error("Unable to derive oAuthPubKey");
    }
    const oAuthX = oAuthPubKey.getX().toString(16, 64);
    const oAuthY = oAuthPubKey.getY().toString(16, 64);
    const oAuthAddress = generateAddressFromPubKey(this.keyType, oAuthPubKey.getX(), oAuthPubKey.getY());
    if (!finalPubKey) {
      throw new Error("Unable to derive finalPubKey");
    }
    const finalX = finalPubKey ? finalPubKey.getX().toString(16, 64) : "";
    const finalY = finalPubKey ? finalPubKey.getY().toString(16, 64) : "";
    const finalAddress = finalPubKey ? generateAddressFromPubKey(this.keyType, finalPubKey.getX(), finalPubKey.getY()) : "";
    return {
      oAuthKeyData: {
        walletAddress: oAuthAddress,
        X: oAuthX,
        Y: oAuthY
      },
      finalKeyData: {
        walletAddress: finalAddress,
        X: finalX,
        Y: finalY
      },
      metadata: {
        pubNonce,
        nonce,
        upgraded: (nonceResult === null || nonceResult === void 0 ? void 0 : nonceResult.upgraded) || false,
        typeOfUser: "v2",
        serverTimeOffset: finalServerTimeOffset
      },
      nodesData: {
        nodeIndexes
      }
    };
  }
  async formatLegacyPublicKeyData(params) {
    var _nonceResult;
    const {
      finalKeyResult,
      enableOneKey,
      isNewKey,
      serverTimeOffset
    } = params;
    const {
      pub_key_X: X,
      pub_key_Y: Y
    } = finalKeyResult.keys[0];
    let nonceResult;
    let nonce;
    let finalPubKey;
    let typeOfUser;
    let pubNonce;
    const oAuthPubKey = this.ec.keyFromPublic({
      x: X,
      y: Y
    }).getPublic();
    const finalServerTimeOffset = this.serverTimeOffset || serverTimeOffset;
    if (enableOneKey) {
      try {
        nonceResult = await getOrSetNonce(this.legacyMetadataHost, this.ec, finalServerTimeOffset, X, Y, undefined, !isNewKey);
        nonce = new BN$1(nonceResult.nonce || "0", 16);
        typeOfUser = nonceResult.typeOfUser;
      } catch {
        throw new GetOrSetNonceError();
      }
      if (nonceResult.typeOfUser === "v1") {
        nonce = await getMetadata(this.legacyMetadataHost, {
          pub_key_X: X,
          pub_key_Y: Y
        });
        finalPubKey = this.ec.keyFromPublic({
          x: X,
          y: Y
        }).getPublic().add(this.ec.keyFromPrivate(nonce.toString(16, 64), "hex").getPublic());
      } else if (nonceResult.typeOfUser === "v2") {
        finalPubKey = this.ec.keyFromPublic({
          x: X,
          y: Y
        }).getPublic().add(this.ec.keyFromPublic({
          x: nonceResult.pubNonce.x,
          y: nonceResult.pubNonce.y
        }).getPublic());
        pubNonce = {
          X: nonceResult.pubNonce.x,
          Y: nonceResult.pubNonce.y
        };
      } else {
        throw new Error("getOrSetNonce should always return typeOfUser.");
      }
    } else {
      typeOfUser = "v1";
      nonce = await getMetadata(this.legacyMetadataHost, {
        pub_key_X: X,
        pub_key_Y: Y
      });
      finalPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic().add(this.ec.keyFromPrivate(nonce.toString(16, 64), "hex").getPublic());
    }
    if (!oAuthPubKey) {
      throw new Error("Unable to derive oAuthPubKey");
    }
    const oAuthX = oAuthPubKey.getX().toString(16, 64);
    const oAuthY = oAuthPubKey.getY().toString(16, 64);
    const oAuthAddress = generateAddressFromPubKey(this.keyType, oAuthPubKey.getX(), oAuthPubKey.getY());
    if (typeOfUser === "v2" && !finalPubKey) {
      throw new Error("Unable to derive finalPubKey");
    }
    const finalX = finalPubKey ? finalPubKey.getX().toString(16, 64) : "";
    const finalY = finalPubKey ? finalPubKey.getY().toString(16, 64) : "";
    const finalAddress = finalPubKey ? generateAddressFromPubKey(this.keyType, finalPubKey.getX(), finalPubKey.getY()) : "";
    return {
      oAuthKeyData: {
        walletAddress: oAuthAddress,
        X: oAuthX,
        Y: oAuthY
      },
      finalKeyData: {
        walletAddress: finalAddress,
        X: finalX,
        Y: finalY
      },
      metadata: {
        pubNonce,
        nonce,
        upgraded: ((_nonceResult = nonceResult) === null || _nonceResult === void 0 ? void 0 : _nonceResult.upgraded) || false,
        typeOfUser,
        serverTimeOffset: finalServerTimeOffset
      },
      nodesData: {
        nodeIndexes: []
      }
    };
  }
}
_defineProperty(Torus, "sessionTime", 86400);

export { GetOrSetNonceError, GetOrSetTssDKGPubKey, GetPubKeyOrKeyAssign, JRPC_METHODS, Point, Polynomial, SAPPHIRE_DEVNET_METADATA_URL, SAPPHIRE_METADATA_URL, Share, Torus, VerifierLookupRequest, calculateMedian, convertMetadataToNonce, decryptNodeData, decryptNodeDataWithPadding, decryptSeedData, derivePubKey, encParamsBufToHex, encParamsHexToBuf, encodeEd25519Point, generateAddressFromPrivKey, generateAddressFromPubKey, generateEd25519KeyData, generateMetadataParams, generateNonceMetadataParams, generatePrivateKey, generateRandomPolynomial, generateSecp256k1KeyData, generateShares, getEd25519ExtendedPublicKey, getEncryptionEC, getKeyCurve, getMetadata, getNonce, getOrSetNonce, getOrSetSapphireMetadataNonce, getPostboxKeyFrom1OutOf1, getProxyCoordinatorEndpointIndex, getSecpKeyFromEd25519, kCombinations, keccak256, lagrangeInterpolatePolynomial, lagrangeInterpolation, normalizeKeysResult, normalizeLookUpResult, retrieveOrImportShare, retryCommitment, stripHexPrefix, thresholdSame, toChecksumAddress, waitFor };
